{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.nn import ReLU\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# from misc_functions import *\n",
    "from model import *\n",
    "from dataset import VNOnDB, get_data_loader\n",
    "from utils import ScaleImageByHeight\n",
    "\n",
    "device = f'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('runs/tf_ctc_word_resnet18.pt', map_location=device)\n",
    "root_config = checkpoint['config']\n",
    "best_metrics = dict()\n",
    "config = root_config['common']\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "        ImageOps.invert,\n",
    "        ScaleImageByHeight(config['scale_height']),\n",
    "        transforms.Grayscale(3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "loader = get_data_loader(config['dataset'],\n",
    "                             'test',\n",
    "                             config['batch_size'],\n",
    "                             2,\n",
    "                             image_transform,\n",
    "                             False,\n",
    "                             flatten_type=config.get('flatten_type', None),\n",
    "                             add_blank=True) # CTC need add_blank\n",
    "\n",
    "if config['dataset'] in ['vnondb', 'vnondb_line']:\n",
    "    vocab = VNOnDB.vocab\n",
    "\n",
    "cnn = ResnetFE('resnet18')\n",
    "\n",
    "model_config = root_config['tf']\n",
    "model = CTCModelTFEncoder(cnn, vocab, model_config)\n",
    "\n",
    "model.to(device)\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder = 'misc'\n",
    "if not os.path.exists(img_folder):\n",
    "    os.mkdir(img_folder)\n",
    "    \n",
    "fp = 'data/VNOnDB/word/test_word/20151208_0146_7105_1_tg_0_0_6.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_test_loader = iter(loader)\n",
    "batch = next(iter_test_loader)\n",
    "imgs, targets = batch.images.to(device), batch.labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 72)\n",
      "torch.Size([3, 96, 170])\n"
     ]
    }
   ],
   "source": [
    "original_img = Image.open(fp).convert('RGB')\n",
    "print(original_img.size)\n",
    "img = image_transform(original_img)\n",
    "print(img.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 96, 170])\n",
      "torch.Size([1, 64, 48, 85])\n",
      "torch.Size([1, 64, 48, 85])\n",
      "torch.Size([1, 64, 48, 85])\n",
      "torch.Size([1, 64, 24, 43])\n",
      "torch.Size([1, 64, 24, 43])\n",
      "torch.Size([1, 128, 12, 22])\n",
      "torch.Size([1, 256, 6, 11])\n",
      "torch.Size([1, 512, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = img.unsqueeze(0)\n",
    "    print(x.size())\n",
    "    for index, layer in enumerate(model.cnn.cnn):\n",
    "        x = layer(x)\n",
    "        print(x.size())\n",
    "        grid_x = x.reshape(-1, 1, x.size(2), x.size(3))\n",
    "        grid = torchvision.utils.make_grid(grid_x, nrow=int(grid_x.size(0)/8))\n",
    "        torchvision.utils.save_image(grid, '{}/cnn_layer_{}.png'.format(img_folder, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer activation with guided backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedBackprop():\n",
    "    \"\"\"\n",
    "       Produces gradients generated with guided back propagation from the given image\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.gradients = None\n",
    "        self.forward_relu_outputs = []\n",
    "        # Put model in evaluation mode\n",
    "        self.model.eval()\n",
    "        self.update_relus()\n",
    "        self.hook_layers()\n",
    "\n",
    "    def hook_layers(self):\n",
    "        def hook_function(module, grad_in, grad_out):\n",
    "            self.gradients = grad_in[0]\n",
    "        # Register hook to the first layer\n",
    "        first_layer = self.model.cnn.cnn[0]\n",
    "        first_layer.register_backward_hook(hook_function)\n",
    "\n",
    "    def update_relus(self):\n",
    "        \"\"\"\n",
    "            Updates relu activation functions so that\n",
    "                1- stores output in forward pass\n",
    "                2- imputes zero for gradient values that are less than zero\n",
    "        \"\"\"\n",
    "        def relu_backward_hook_function(module, grad_in, grad_out):\n",
    "            \"\"\"\n",
    "            If there is a negative gradient, change it to zero\n",
    "            \"\"\"\n",
    "            # Get last forward output\n",
    "            corresponding_forward_output = self.forward_relu_outputs[-1]\n",
    "            corresponding_forward_output[corresponding_forward_output > 0] = 1\n",
    "            modified_grad_out = corresponding_forward_output * torch.clamp(grad_in[0], min=0.0)\n",
    "            del self.forward_relu_outputs[-1]  # Remove last forward output\n",
    "            return (modified_grad_out,)\n",
    "\n",
    "        def relu_forward_hook_function(module, ten_in, ten_out):\n",
    "            \"\"\"\n",
    "            Store results of forward pass\n",
    "            \"\"\"\n",
    "            self.forward_relu_outputs.append(ten_out)\n",
    "\n",
    "        # Loop through layers, hook up ReLUs\n",
    "        for pos, module in self.model.cnn.cnn._modules.items():\n",
    "            if isinstance(module, ReLU):\n",
    "                module.register_backward_hook(relu_backward_hook_function)\n",
    "                module.register_forward_hook(relu_forward_hook_function)\n",
    "\n",
    "    def generate_gradients(self, input_image, cnn_layer, filter_pos):\n",
    "        self.model.zero_grad()\n",
    "        # Forward pass\n",
    "        x = input_image\n",
    "        for index, layer in enumerate(self.model.cnn.cnn):\n",
    "            # Forward pass layer by layer\n",
    "            # x is not used after this point because it is only needed to trigger\n",
    "            # the forward hook function\n",
    "            x = layer(x)\n",
    "            # Only need to forward until the selected layer is reached\n",
    "            if index == cnn_layer:\n",
    "                # (forward hook function triggered)\n",
    "                break\n",
    "        conv_output = torch.sum(torch.abs(x[0, filter_pos]))\n",
    "        # Backward pass\n",
    "        conv_output.backward()\n",
    "        # Convert Pytorch variable to numpy array\n",
    "        # [0] to get rid of the first channel (1,3,224,224)\n",
    "        gradients_as_arr = self.gradients.data.numpy()[0]\n",
    "        return gradients_as_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 72)\n",
      "torch.Size([1, 3, 96, 170])\n"
     ]
    }
   ],
   "source": [
    "original_image = Image.open(fp).convert('RGB')\n",
    "print(original_image.size)\n",
    "prep_img = image_transform(original_image)\n",
    "prep_img.unsqueeze_(0)\n",
    "prep_img = Variable(prep_img, requires_grad=True)\n",
    "print(prep_img.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = [64, 64, 64, 64, 64, 128, 256, 512]\n",
    "\n",
    "for cnn_layer_idx in range(7):\n",
    "    grads = []\n",
    "    for filter_pos in range(n_filters[cnn_layer_idx]):\n",
    "        # Guided backprop\n",
    "        GBP = GuidedBackprop(model)\n",
    "        # Get gradients\n",
    "        guided_grads = GBP.generate_gradients(prep_img, cnn_layer_idx, filter_pos)\n",
    "        grads.append(guided_grads)\n",
    "\n",
    "    grads_tensor = torch.tensor(grads)\n",
    "    grid = torchvision.utils.make_grid(grads_tensor, nrow=8)\n",
    "    torchvision.utils.save_image(grid, '{}/layer_activation_{}.png'.format(img_folder, cnn_layer_idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
