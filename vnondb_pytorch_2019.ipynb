{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RdyIpXsB0yUE"
   },
   "source": [
    "# Load dataset from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p7be0t5-ch_X"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import random\n",
    "import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uh_VdlXsch_c"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-667ecca00de9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_CHAR = '<start>' # start of sequence character\n",
    "EOS_CHAR = '<end>' # end of sequence character\n",
    "PAD_CHAR = '<pad>' # padding character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VNOnDB(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, csv, image_transform=None):\n",
    "        self.df = pd.read_csv(csv, sep='\\t', keep_default_na=False, index_col=0)\n",
    "        self.image_folder = image_folder\n",
    "        self.image_transform = image_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_folder, self.df['id'][idx]+'.png')\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        \n",
    "        label = self.df['label'][idx]\n",
    "        label = [SOS_CHAR] + list(label) + [EOS_CHAR]\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleImageByHeight(object):\n",
    "    def __init__(self, target_height):\n",
    "        self.target_height = target_height\n",
    "\n",
    "    def __call__(self, image):\n",
    "        width, height = image.size\n",
    "        factor = self.target_height / height\n",
    "        new_width = int(width * factor)\n",
    "        new_height = int(height * factor)\n",
    "        image = image.resize((new_width, new_height))\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Grayscale(3),\n",
    "    ScaleImageByHeight(32),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_csv = './data/VNOnDB/all_word.csv'\n",
    "train_data_csv = './data/VNOnDB/train_word.csv'\n",
    "val_data_csv = './data/VNOnDB/validation_word.csv'\n",
    "test_data_csv = './data/VNOnDB/test_word.csv'\n",
    "\n",
    "train_image_folder = './data/VNOnDB/word_train'\n",
    "val_image_folder = './data/VNOnDB/word_val'\n",
    "test_image_folder = './data/VNOnDB/word_test'\n",
    "\n",
    "train_data = VNOnDB(train_image_folder, train_data_csv, image_transform)\n",
    "validation_data = VNOnDB(val_image_folder, val_data_csv, image_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df = pd.read_csv(all_data_csv, sep='\\t', keep_default_na=False, index_col=0)\n",
    "alphabets = sorted(list(set.union(*all_data_df.label.apply(set))) + [SOS_CHAR, EOS_CHAR, PAD_CHAR])\n",
    "\n",
    "char2int = dict((c, i) for i, c in enumerate(alphabets))\n",
    "int2char = dict((i, c) for i, c in enumerate(alphabets))\n",
    "vocab_size = len(alphabets)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(samples):\n",
    "    '''\n",
    "    :param samples: list of tuples:\n",
    "        - image: tensor of [C, H, W]\n",
    "        - label: list of characters including '<start>' and '<end>' at both ends\n",
    "    :returns:\n",
    "        - images: tensor of [B, C, H, W]\n",
    "        - labels: tensor of [max_T, B, 1]\n",
    "        - lengths: tensor of [B, 1]\n",
    "    '''\n",
    "    batch_size = len(samples)\n",
    "    samples.sort(key=lambda sample: len(sample[1]), reverse=True)\n",
    "    image_samples, label_samples = list(zip(*samples))\n",
    "\n",
    "    # images: [B, 3, H, W]\n",
    "    max_image_row = max([image.size(1) for image in image_samples])\n",
    "    max_image_col = max([image.size(2) for image in image_samples])\n",
    "    images = torch.ones(batch_size, 3, max_image_row, max_image_col)\n",
    "    for i, image in enumerate(image_samples):\n",
    "        image_row = image.shape[1]\n",
    "        image_col = image.shape[2]\n",
    "        images[i, :, :image_row, :image_col] = image\n",
    "\n",
    "    label_lengths = [len(label) for label in label_samples]\n",
    "    max_length = max(label_lengths)\n",
    "    label_samples = [label + [PAD_CHAR] * (max_length - len(label)) for label in label_samples]\n",
    "    \n",
    "    labels = torch.zeros(max(label_lengths), batch_size, 1, dtype=torch.long) # [max_T, B, 1]\n",
    "    for i, label in enumerate(label_samples):\n",
    "        label_int = torch.tensor([char2int[char] for char in label]).view(-1, 1) # [T, 1]\n",
    "        labels[:, i] = label_int\n",
    "        \n",
    "    labels_onehot = torch.zeros(max(label_lengths), batch_size, vocab_size, dtype=torch.long) # [max_T, B, vocab_size]\n",
    "    for label_i, label in enumerate(label_samples):\n",
    "        for char_i, char in enumerate(label):\n",
    "            char_int = char2int[char]\n",
    "            onehot = torch.zeros(vocab_size, dtype=torch.long)\n",
    "            onehot[char_int] = 1\n",
    "            labels_onehot[char_i, label_i] = onehot\n",
    "\n",
    "    return images, labels, labels_onehot, torch.tensor(label_lengths).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, depth, n_blocks, growth_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.cnn = torchvision.models.DenseNet(\n",
    "            growth_rate=growth_rate,\n",
    "            block_config=[depth]*n_blocks\n",
    "        ).features\n",
    "\n",
    "        # TODO: fix me\n",
    "        self.n_features = self.cnn.norm5.num_features\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        :param inputs: [B, C, H, W]\n",
    "        :returms: [num_pixels, B, C']\n",
    "        '''\n",
    "        batch_size = inputs.size(0)\n",
    "        outputs = self.cnn(inputs) # [B, C', H', W']\n",
    "        outputs = outputs.view(batch_size, self.n_features, -1) # [B, C', H' x W'] == [B, C', num_pixels]\n",
    "        outputs = outputs.permute(2, 0, 1) # [num_pixels, B, C']\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size, attn_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.Wa = nn.Linear(feature_size, attn_size)\n",
    "        self.Ua = nn.Linear(hidden_size, attn_size)\n",
    "        self.va = nn.Linear(attn_size, 1)\n",
    "\n",
    "    def forward(self, last_hidden, encoder_outputs):\n",
    "        '''\n",
    "        Input:\n",
    "        :param last_hidden: [1, B, H]\n",
    "        :param encoder_outputs: [num_pixels, B, C]\n",
    "        Output:\n",
    "        weights: [num_pixels, B, 1]\n",
    "        '''\n",
    "        attn1 = self.Wa(encoder_outputs) # [num_pixels, B, A]\n",
    "        attn2 = self.Ua(last_hidden) # [1, B, A]\n",
    "        attn = self.va(torch.tanh(attn1 + attn2)) # [num_pixels, B, 1]\n",
    "        \n",
    "        weights = F.softmax(attn.squeeze(2), 1).unsqueeze(2) # [num_pixels, B, 1]\n",
    "        context = (weights * encoder_outputs).sum(0, keepdim=True) # [1, B, C]\n",
    "        \n",
    "        return context, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size, vocab_size, attn_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.feature_size = feature_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.attn_size = attn_size\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.vocab_size+self.feature_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(\n",
    "            self.feature_size,\n",
    "            self.hidden_size,\n",
    "            self.attn_size)\n",
    "\n",
    "        self.character_distribution = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, img_features, targets, teacher_forcing_ratio=0.5):\n",
    "        '''\n",
    "        :param img_features: tensor of [num_pixels, B, C]\n",
    "        :param targets: tensor of [T, B, V], each target has <start> and <end> at begin and end of the word\n",
    "        :return:\n",
    "            outputs: tensor of [T, B, V]\n",
    "            weights: tensor of [T, B, num_pixels]\n",
    "        '''\n",
    "\n",
    "        num_pixels = img_features.size(0)\n",
    "        batch_size = img_features.size(1)\n",
    "        max_length = targets.size(0)\n",
    "\n",
    "        targets = targets.float()\n",
    "        rnn_input = targets[[0]].float() # [1, B, V]\n",
    "        hidden = self.init_hidden(batch_size).to(img_features.device)\n",
    "\n",
    "        outputs = torch.zeros(max_length, batch_size, self.vocab_size, device=img_features.device)\n",
    "        weights = torch.zeros(max_length, batch_size, num_pixels, device=img_features.device) \n",
    "\n",
    "        # pdb.set_trace()\n",
    "        for t in range(max_length - 1):\n",
    "            context, weight = self.attention(hidden, img_features) # [1, B, C], [num_pixels, B, 1]\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            if self.training and teacher_force:\n",
    "                rnn_input = torch.cat((targets[[t]], context), -1)\n",
    "            else:\n",
    "                rnn_input = torch.cat((rnn_input, context), -1)\n",
    "\n",
    "            output, hidden = self.rnn(rnn_input, hidden)\n",
    "            output = self.character_distribution(output)\n",
    "\n",
    "            outputs[[t]] = output\n",
    "            weights[[t]] = weight.transpose(0, 2)\n",
    "            \n",
    "            rnn_input = output\n",
    "            \n",
    "        return outputs, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  'batch_size': 64,\n",
    "  'hidden_size': 256,\n",
    "  'attn_size': 256,\n",
    "  'max_length': 10,\n",
    "  'n_epochs_decrease_lr': 15,\n",
    "  'start_learning_rate': 1e-3, # NOTE: paper start with 1e-8\n",
    "  'end_learning_rate': 1e-11,\n",
    "  'depth': 4,\n",
    "  'n_blocks': 3,\n",
    "  'growth_rate': 96,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True, collate_fn=collate_fn, num_workers=1)\n",
    "val_loader = DataLoader(validation_data, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(config['depth'], config['n_blocks'], config['growth_rate'])\n",
    "\n",
    "decoder = Decoder(encoder.n_features, config['hidden_size'], vocab_size, config['attn_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = optim.Adam(params, lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "t = tqdm.tqdm(train_loader, desc='Training', ascii=True)\n",
    "\n",
    "for (imgs, targets, targets_onehot, lengths) in t:\n",
    "    img_features = encoder(imgs)\n",
    "    outputs, weights = decoder(img_features, targets_onehot)\n",
    "    _, predicts = outputs.softmax(-1).max(-1, keepdim=True)\n",
    "\n",
    "    packed_outputs = torch.nn.utils.rnn.pack_padded_sequence(outputs, lengths.squeeze())[0]\n",
    "    packed_targets = torch.nn.utils.rnn.pack_padded_sequence(targets.squeeze(), lengths.squeeze())[0]\n",
    "    loss = criterion(packed_outputs, packed_targets)\n",
    "    \n",
    "    t.set_postfix(loss=loss.item())\n",
    "    t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of gated_attention.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
