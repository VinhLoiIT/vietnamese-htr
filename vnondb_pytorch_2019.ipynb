{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RdyIpXsB0yUE"
   },
   "source": [
    "# Load dataset from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p7be0t5-ch_X"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uh_VdlXsch_c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_CHAR = '<start>' # start of sequence character\n",
    "EOS_CHAR = '<end>' # end of sequence character\n",
    "PAD_CHAR = '<pad>' # padding character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VNOnDB(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, csv, image_transform=None):\n",
    "        self.df = pd.read_csv(csv, sep='\\t', keep_default_na=False, index_col=0)\n",
    "        self.image_folder = image_folder\n",
    "        self.image_transform = image_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_folder, self.df['id'][idx]+'.png')\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        \n",
    "        label = self.df['label'][idx]\n",
    "        label = [SOS_CHAR] + list(label) + [EOS_CHAR]\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleImageByHeight(object):\n",
    "    def __init__(self, target_height):\n",
    "        self.target_height = target_height\n",
    "\n",
    "    def __call__(self, image):\n",
    "        width, height = image.size\n",
    "        factor = self.target_height / height\n",
    "        new_width = int(width * factor)\n",
    "        new_height = int(height * factor)\n",
    "        image = image.resize((new_width, new_height))\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Grayscale(3),\n",
    "    ScaleImageByHeight(32),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_csv = './data/VNOnDB/all_word.csv'\n",
    "train_data_csv = './data/VNOnDB/train_word.csv'\n",
    "val_data_csv = './data/VNOnDB/validation_word.csv'\n",
    "test_data_csv = './data/VNOnDB/test_word.csv'\n",
    "\n",
    "train_image_folder = './data/VNOnDB/word_train'\n",
    "val_image_folder = './data/VNOnDB/word_val'\n",
    "test_image_folder = './data/VNOnDB/word_test'\n",
    "\n",
    "train_data = VNOnDB(train_image_folder, train_data_csv, image_transform)\n",
    "validation_data = VNOnDB(val_image_folder, val_data_csv, image_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df = pd.read_csv(all_data_csv, sep='\\t', keep_default_na=False, index_col=0)\n",
    "alphabets = sorted(list(set.union(*all_data_df.label.apply(set))) + [SOS_CHAR, EOS_CHAR, PAD_CHAR])\n",
    "\n",
    "char2int = dict((c, i) for i, c in enumerate(alphabets))\n",
    "int2char = dict((i, c) for i, c in enumerate(alphabets))\n",
    "vocab_size = len(alphabets)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(samples):\n",
    "    '''\n",
    "    :param samples: list of tuples:\n",
    "        - image: tensor of [C, H, W]\n",
    "        - label: list of characters including '<start>' and '<end>' at both ends\n",
    "    :returns:\n",
    "        - images: tensor of [B, C, H, W]\n",
    "        - labels: tensor of [max_T, B, 1]\n",
    "        - lengths: tensor of [B, 1]\n",
    "    '''\n",
    "    batch_size = len(samples)\n",
    "    samples.sort(key=lambda sample: len(sample[1]), reverse=True)\n",
    "    image_samples, label_samples = list(zip(*samples))\n",
    "\n",
    "    # images: [B, 3, H, W]\n",
    "    max_image_row = max([image.size(1) for image in image_samples])\n",
    "    max_image_col = max([image.size(2) for image in image_samples])\n",
    "    images = torch.ones(batch_size, 3, max_image_row, max_image_col)\n",
    "    for i, image in enumerate(image_samples):\n",
    "        image_row = image.shape[1]\n",
    "        image_col = image.shape[2]\n",
    "        images[i, :, :image_row, :image_col] = image\n",
    "\n",
    "    label_lengths = [len(label) for label in label_samples]\n",
    "    max_length = max(label_lengths)\n",
    "    label_samples = [label + [PAD_CHAR] * (max_length - len(label)) for label in label_samples]\n",
    "    \n",
    "    labels = torch.zeros(max(label_lengths), batch_size, 1, dtype=torch.long) # [max_T, B, 1]\n",
    "    for i, label in enumerate(label_samples):\n",
    "        label_int = torch.tensor([char2int[char] for char in label]).view(-1, 1) # [T, 1]\n",
    "        labels[:, i] = label_int\n",
    "        \n",
    "    labels_onehot = torch.zeros(max(label_lengths), batch_size, vocab_size, dtype=torch.long) # [max_T, B, vocab_size]\n",
    "    for label_i, label in enumerate(label_samples):\n",
    "        for char_i, char in enumerate(label):\n",
    "            char_int = char2int[char]\n",
    "            onehot = torch.zeros(vocab_size, dtype=torch.long)\n",
    "            onehot[char_int] = 1\n",
    "            labels_onehot[char_i, label_i] = onehot\n",
    "\n",
    "    return images, labels, labels_onehot, torch.tensor(label_lengths).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, depth, n_blocks, growth_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.cnn = torchvision.models.DenseNet(\n",
    "            growth_rate=growth_rate,\n",
    "            block_config=[depth]*n_blocks\n",
    "        ).features\n",
    "\n",
    "        # TODO: fix me\n",
    "        self.n_features = self.cnn.norm5.num_features\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        :param inputs: [B, C, H, W]\n",
    "        :returms: [num_pixels, B, C']\n",
    "        '''\n",
    "        batch_size = inputs.size(0)\n",
    "        outputs = self.cnn(inputs) # [B, C', H', W']\n",
    "        outputs = outputs.view(batch_size, self.n_features, -1) # [B, C', H' x W'] == [B, C', num_pixels]\n",
    "        outputs = outputs.permute(2, 0, 1) # [num_pixels, B, C']\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size, attn_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.Wa = nn.Linear(feature_size, attn_size)\n",
    "        self.Ua = nn.Linear(hidden_size, attn_size)\n",
    "        self.va = nn.Linear(attn_size, 1)\n",
    "\n",
    "    def forward(self, last_hidden, encoder_outputs):\n",
    "        '''\n",
    "        Input:\n",
    "        :param last_hidden: [1, B, H]\n",
    "        :param encoder_outputs: [num_pixels, B, C]\n",
    "        Output:\n",
    "        weights: [num_pixels, B, 1]\n",
    "        '''\n",
    "        attn1 = self.Wa(encoder_outputs) # [num_pixels, B, A]\n",
    "        attn2 = self.Ua(last_hidden) # [1, B, A]\n",
    "        attn = self.va(torch.tanh(attn1 + attn2)) # [num_pixels, B, 1]\n",
    "        \n",
    "        weights = F.softmax(attn.squeeze(2), 1).unsqueeze(2) # [num_pixels, B, 1]\n",
    "        context = (weights * encoder_outputs).sum(0, keepdim=True) # [1, B, C]\n",
    "        \n",
    "        return context, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size, vocab_size, attn_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.feature_size = feature_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.attn_size = attn_size\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.vocab_size+self.feature_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(\n",
    "            self.feature_size,\n",
    "            self.hidden_size,\n",
    "            self.attn_size)\n",
    "\n",
    "        self.character_distribution = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, img_features, targets, teacher_forcing_ratio=0.5):\n",
    "        '''\n",
    "        :param img_features: tensor of [num_pixels, B, C]\n",
    "        :param targets: tensor of [T, B, V], each target has <start> and <end> at begin and end of the word\n",
    "        :return:\n",
    "            outputs: tensor of [T, B, V]\n",
    "            weights: tensor of [T, B, num_pixels]\n",
    "        '''\n",
    "\n",
    "        num_pixels = img_features.size(0)\n",
    "        batch_size = img_features.size(1)\n",
    "        max_length = targets.size(0)\n",
    "\n",
    "        targets = targets.float()\n",
    "        rnn_input = targets[[0]].float() # [1, B, V]\n",
    "        hidden = self.init_hidden(batch_size).to(img_features.device)\n",
    "\n",
    "        outputs = torch.zeros(max_length, batch_size, self.vocab_size, device=img_features.device)\n",
    "        weights = torch.zeros(max_length, batch_size, num_pixels, device=img_features.device) \n",
    "\n",
    "        # pdb.set_trace()\n",
    "        for t in range(max_length - 1):\n",
    "            context, weight = self.attention(hidden, img_features) # [1, B, C], [num_pixels, B, 1]\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            if self.training and teacher_force:\n",
    "                rnn_input = torch.cat((targets[[t]], context), -1)\n",
    "            else:\n",
    "                rnn_input = torch.cat((rnn_input, context), -1)\n",
    "\n",
    "            output, hidden = self.rnn(rnn_input, hidden)\n",
    "            output = self.character_distribution(output)\n",
    "\n",
    "            outputs[[t]] = output\n",
    "            weights[[t]] = weight.transpose(0, 2)\n",
    "            \n",
    "            rnn_input = output\n",
    "            \n",
    "        return outputs, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, targets):\n",
    "    batch_size = outputs.size(0)\n",
    "    _, ind = outputs.topk(1, 1, True, True)\n",
    "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
    "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
    "    return correct_total.item() / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, train_loader, encoder, decoder, optimizer, criterion, writer, log_interval=100):\n",
    "    global train_step\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    accs = AverageMeter()\n",
    "    \n",
    "    for i, (imgs, targets, targets_onehot, lengths) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        targets_onehot = targets_onehot.to(device)\n",
    "\n",
    "        img_features = encoder(imgs)\n",
    "        outputs, weights = decoder(img_features, targets_onehot)\n",
    "\n",
    "        packed_outputs = torch.nn.utils.rnn.pack_padded_sequence(outputs, lengths.squeeze())[0]\n",
    "        packed_targets = torch.nn.utils.rnn.pack_padded_sequence(targets.squeeze(), lengths.squeeze())[0]\n",
    "        \n",
    "        loss = criterion(packed_outputs, packed_targets)\n",
    "        acc = accuracy(packed_outputs, packed_targets)\n",
    "        \n",
    "        total_characters = lengths.sum().item()\n",
    "        losses.update(loss, total_characters)\n",
    "        accs.update(acc, total_characters)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_step += 1\n",
    "        writer.add_scalar('Train/Loss', loss.item(), train_step)\n",
    "        writer.add_scalar('Train/Accuracy', acc, train_step)\n",
    "        \n",
    "        if (i+1) % log_interval == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Accuracy {accs.val:.3f} ({accs.avg:.3f})'.format(epoch, i, len(train_loader),\n",
    "                                                                    loss=losses,\n",
    "                                                                    accs=accs))\n",
    "    return losses.avg, accs.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, val_loader, encoder, decoder, criterion, writer, log_interval=100):\n",
    "    global val_step\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    accs = AverageMeter()\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, targets, targets_onehot, lengths) in enumerate(val_loader):\n",
    "\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            targets_onehot = targets_onehot.to(device)\n",
    "\n",
    "            img_features = encoder(imgs)\n",
    "            outputs, weights = decoder(img_features, targets_onehot)\n",
    "            \n",
    "\n",
    "            packed_outputs = torch.nn.utils.rnn.pack_padded_sequence(outputs, lengths.squeeze())[0]\n",
    "            packed_targets = torch.nn.utils.rnn.pack_padded_sequence(targets.squeeze(), lengths.squeeze())[0]\n",
    "            loss = criterion(packed_outputs, packed_targets)\n",
    "            acc = accuracy(packed_outputs, packed_targets)\n",
    "\n",
    "            total_characters = lengths.sum().item()\n",
    "            losses.update(loss, total_characters)\n",
    "            accs.update(acc, total_characters)\n",
    "\n",
    "            val_step += 1\n",
    "            writer.add_scalar('Validate/Loss', loss.item(), val_step)\n",
    "            writer.add_scalar('Validate/Accuracy', acc, val_step)\n",
    "\n",
    "            if (i+1) % log_interval == 0:\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Accuracy {accs.val:.3f} ({accs.avg:.3f})'.format(epoch, i, len(val_loader),\n",
    "                                                                        loss=losses,\n",
    "                                                                        accs=accs))\n",
    "    return losses.avg, accs.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, train_step, val_step, encoder, decoder, optimizer, lr, best_val_acc, is_best=False):\n",
    "    info = {\n",
    "        'epoch': epoch,\n",
    "        'train_step': train_step,\n",
    "        'val_step': val_step,\n",
    "        'encoder_state': encoder.state_dict(),\n",
    "        'decoder_state': decoder.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict(),\n",
    "        'lr': lr,\n",
    "        'best_val_acc': best_val_acc,\n",
    "    }\n",
    "    torch.save('./ckpt/weights.pt')\n",
    "    if is_best:\n",
    "        torch.save('./ckpt/best_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  'batch_size': 64,\n",
    "  'hidden_size': 256,\n",
    "  'attn_size': 256,\n",
    "  'max_length': 10,\n",
    "  'n_epochs_decrease_lr': 15,\n",
    "  'start_learning_rate': 1e-5, # NOTE: paper start with 1e-8\n",
    "  'end_learning_rate': 1e-11,\n",
    "  'depth': 4,\n",
    "  'n_blocks': 3,\n",
    "  'growth_rate': 96,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True, collate_fn=collate_fn, num_workers=8)\n",
    "val_loader = DataLoader(validation_data, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(config['depth'], config['n_blocks'], config['growth_rate']).to(device)\n",
    "decoder = Decoder(encoder.n_features, config['hidden_size'], vocab_size, config['attn_size']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = optim.Adam(params, lr=config['start_learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "train_step = 0\n",
    "val_step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][199/1047]\tLoss 4.2969 (4.7703)\tAccuracy 0.181 (0.133)\n",
      "Epoch: [1][399/1047]\tLoss 3.5187 (4.3190)\tAccuracy 0.370 (0.204)\n",
      "Epoch: [1][599/1047]\tLoss 3.1056 (3.9839)\tAccuracy 0.372 (0.257)\n",
      "Epoch: [1][799/1047]\tLoss 2.8779 (3.7405)\tAccuracy 0.367 (0.284)\n"
     ]
    }
   ],
   "source": [
    "log_interval = 200\n",
    "epoch = 0\n",
    "best_val_acc = 0\n",
    "count_decrease_lr = 0\n",
    "lr = config['start_learning_rate']\n",
    "while True:\n",
    "    epoch += 1 \n",
    "    train_loss, train_acc = train_one_epoch(epoch, train_loader, encoder, decoder, optimizer, criterion, writer, log_interval)\n",
    "    val_loss, val_acc = validate(epoch, val_loader, encoder, decoder, criterion, writer, log_interval)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        count_decrease_lr = 0\n",
    "        save_checkpoint(epoch, train_step, val_step, encoder, decoder, optimizer, lr, best_val_acc, True)\n",
    "    else:\n",
    "        count_decrease_lr += 1\n",
    "        if count_decrease_lr == config['n_epochs_decrease_lr']:\n",
    "            lr = lr * 0.1\n",
    "            print('Decrease learning rate to', lr)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            count_decrease_lr = 0\n",
    "\n",
    "    save_checkpoint(epoch, train_step, val_step, encoder, decoder, optimizer, lr, best_val_acc, False)\n",
    "    if lr <= config['end_learning_rate']:\n",
    "        print('Training done')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of gated_attention.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
