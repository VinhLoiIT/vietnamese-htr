{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from dataset import get_data_loader, VNOnDB, RIMES\n",
    "from utils import ScaleImageByHeight, StringTransform\n",
    "\n",
    "from PIL import ImageOps\n",
    "\n",
    "# Reproducible\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "device = f'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class CTCModel(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__()\n",
    "\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, None))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=resnet.fc.in_features, nhead=8)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, 1)\n",
    "        self.character_distribution = nn.Linear(resnet.fc.in_features, vocab.size)\n",
    "\n",
    "    def forward(self, images) -> torch.Tensor:\n",
    "        '''\n",
    "        Shapes:\n",
    "        -------\n",
    "            images: (N,C,H,W)\n",
    "        '''\n",
    "        images = self.cnn(images) # [B,C,H',W']\n",
    "        images = self.pool(images) # [B,C,1,W']\n",
    "        images.squeeze_(-2) # [B,C,W']\n",
    "        images = images.permute(2,0,1) # [S=W',B,C]\n",
    "        images = self.encoder(images) # [S,B,C]\n",
    "        images = images.transpose(0,1) # [B,S,C]\n",
    "        images = self.character_distribution(images) # [B,S,V]\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CTCModel(\n  (cnn): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (pool): AdaptiveAvgPool2d(output_size=(1, None))\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (character_distribution): Linear(in_features=512, out_features=84, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "weight_path = 'runs/06-04-2020_11-43-53_ctc/weights/weights_epoch=6_loss=0.412.pt'\n",
    "\n",
    "checkpoint = torch.load(weight_path, map_location=device)\n",
    "root_config = checkpoint['config']\n",
    "\n",
    "config = root_config['common']\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    ImageOps.invert,\n",
    "    ScaleImageByHeight(config['scale_height']),\n",
    "    transforms.Grayscale(3),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_loader = get_data_loader(config['dataset'],\n",
    "                                'test',\n",
    "                                config['batch_size'],\n",
    "                                1,\n",
    "                                image_transform,\n",
    "                                False,\n",
    "                                flatten_type=config.get('flatten_type', None))\n",
    "\n",
    "if config['dataset'] in ['vnondb', 'vnondb_line']:\n",
    "    vocab = VNOnDB.vocab\n",
    "elif config['dataset'] == 'rimes':\n",
    "    vocab = RIMES.vocab\n",
    "model = CTCModel(vocab)\n",
    "model.to(device)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_loader = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_val(batch):\n",
    "    with torch.no_grad():\n",
    "        imgs, targets = batch.images.to(device), batch.labels.to(device)\n",
    "        targets = targets + 1 # Leave index 0 for '<blank>'\n",
    "\n",
    "        outputs = model(imgs) # [B,S,V]\n",
    "        outputs = F.log_softmax(outputs, -1) # [B,S,V]\n",
    "        outputs_lengths = torch.tensor(outputs.size(1)).expand(outputs.size(0))\n",
    "        return outputs.argmax(-1)\n",
    "\n",
    "outputs = step_val(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 3,  7,  7, 11,  4,  9,  3, 12,  2,  8, 32],\n        [14, 32, 13,  3, 12,  2,  8,  5,  6,  0,  0],\n        [61, 61, 42, 38,  6,  0,  0,  0,  0,  0,  0],\n        [73, 64,  6,  0,  0,  0,  0,  0,  0,  0,  0],\n        [42, 61,  6,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 8,  5,  6,  0,  0,  0,  0,  0,  0,  0,  0],\n        [55, 39,  6,  0,  0,  0,  0,  0,  0,  0,  0],\n        [11,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n"
    }
   ],
   "source": [
    "print(batch.labels[:, 1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int2char_label(labels):\n",
    "    results = []\n",
    "    for label in labels:\n",
    "        label = list(map(RIMES.vocab.int2char, label))\n",
    "        results.append(label[:label.index(RIMES.vocab.EOS)])\n",
    "    return results\n",
    "labels = int2char_label(batch.labels[:,1:].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int2char_output(outputs):\n",
    "    results = []\n",
    "    for output in outputs:\n",
    "        output = [RIMES.vocab.int2char(i - 1) if i != 0 else '<b>' for i in output]\n",
    "        results.append(output)\n",
    "    return results\n",
    "predicts = int2char_output(outputs.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------\n['c', 'o', 'n', 'm', 'm', 'n', 'a', 't', 'i', 'c', 'a', 'l', 'l']\n['i', 'm', 'm', 'a', 't', 'r', 'i', 'c', 'u', 'l', 'é']\n----------\n['v', 'é', 'h', 'i', 'c', 'u', 'u', 'l', 'e', 'e', '<b>', '<b>', '<b>']\n['v', 'é', 'h', 'i', 'c', 'u', 'l', 'e']\n----------\n['7', '<b>', '7', '<b>', '5', '<b>', '2', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>']\n['7', '7', '5', '2']\n----------\n['u', '<b>', 't', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>']\n['W', 'Y']\n----------\n['5', '<b>', '<b>', '7', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>']\n['5', '7']\n----------\n['l', 'e', 'e', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>']\n['l', 'e']\n----------\n['1', '<b>', '<b>', '0', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>']\n['1', '0']\n----------\n['a', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>', '<b>']\n['a']\n"
    }
   ],
   "source": [
    "for pred, tgt in zip(predicts, labels):\n",
    "    print('-'*10)\n",
    "    print(pred)\n",
    "    print(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}