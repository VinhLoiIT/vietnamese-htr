{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from dataset import get_data_loader, VNOnDB, RIMES\n",
    "from utils import ScaleImageByHeight, StringTransform\n",
    "\n",
    "from PIL import ImageOps\n",
    "\n",
    "# Reproducible\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "device = f'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class CTCModel(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__()\n",
    "\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, None))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=resnet.fc.in_features, nhead=8)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, 1)\n",
    "        self.character_distribution = nn.Linear(resnet.fc.in_features, vocab.size)\n",
    "\n",
    "    def forward(self, images) -> torch.Tensor:\n",
    "        '''\n",
    "        Shapes:\n",
    "        -------\n",
    "            images: (N,C,H,W)\n",
    "        '''\n",
    "        images = self.cnn(images) # [B,C,H',W']\n",
    "        images = self.pool(images) # [B,C,1,W']\n",
    "        images.squeeze_(-2) # [B,C,W']\n",
    "        images = images.permute(2,0,1) # [S=W',B,C]\n",
    "        images = self.encoder(images) # [S,B,C]\n",
    "        images = images.transpose(0,1) # [B,S,C]\n",
    "        images = self.character_distribution(images) # [B,S,V]\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CTCModel(\n  (cnn): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (pool): AdaptiveAvgPool2d(output_size=(1, None))\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (character_distribution): Linear(in_features=512, out_features=147, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "weight_path = 'runs/06-04-2020_13-38-17_ctc/weights/weights_epoch=50_loss=0.008.pt'\n",
    "\n",
    "checkpoint = torch.load(weight_path, map_location=device)\n",
    "root_config = checkpoint['config']\n",
    "\n",
    "config = root_config['common']\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    ImageOps.invert,\n",
    "    ScaleImageByHeight(config['scale_height']),\n",
    "    transforms.Grayscale(3),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_loader = get_data_loader(config['dataset'],\n",
    "                                'test',\n",
    "                                config['batch_size'],\n",
    "                                1,\n",
    "                                image_transform,\n",
    "                                False,\n",
    "                                flatten_type=config.get('flatten_type', None))\n",
    "\n",
    "if config['dataset'] in ['vnondb', 'vnondb_line']:\n",
    "    vocab = VNOnDB.vocab\n",
    "elif config['dataset'] == 'rimes':\n",
    "    vocab = RIMES.vocab\n",
    "model = CTCModel(vocab)\n",
    "model.to(device)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_loader = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_val(batch):\n",
    "    with torch.no_grad():\n",
    "        imgs, targets = batch.images.to(device), batch.labels.to(device)\n",
    "        targets = targets + 1 # Leave index 0 for '<blank>'\n",
    "\n",
    "        outputs = model(imgs) # [B,S,V]\n",
    "        outputs = F.log_softmax(outputs, -1) # [B,S,V]\n",
    "        outputs_lengths = torch.tensor(outputs.size(1)).expand(outputs.size(0))\n",
    "        return outputs.argmax(-1)\n",
    "\n",
    "outputs = step_val(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCStringTransform(object):\n",
    "    def __init__(self, vocab, batch_first=True):\n",
    "        self.batch_first = batch_first\n",
    "        self.EOS_int = vocab.char2int(vocab.EOS)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, tensor: torch.tensor):\n",
    "        '''\n",
    "        Convert a Tensor to a list of Strings\n",
    "        '''\n",
    "        if not self.batch_first:\n",
    "            tensor = tensor.transpose(0,1)\n",
    "        # tensor: [B,T]\n",
    "        strs = []\n",
    "        for sample in tensor.tolist():\n",
    "            # sample: [T]\n",
    "            # remove duplicates\n",
    "            sample = [sample[0]] + [c for i,c in enumerate(sample[1:]) if c != sample[i]]\n",
    "            # remove 'blank'\n",
    "            sample = list(filter(lambda i: i != 0, sample))\n",
    "            # fix index\n",
    "            sample = list(map(self.vocab.int2char, np.array(sample) - 1))\n",
    "            strs.append(sample)\n",
    "        return strs\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['h', 'u', 'y', 'ệ', 'n'],\n ['T', 'h', 'à', 'n', 'h'],\n ['n', 'g', 'ư', 'ờ', 'i'],\n ['T', 'i', 'ê', 'n'],\n ['L', 'a', 'n', 'g'],\n ['T', 'i', 'ê', 'n'],\n ['l', 'o', 'n', 'g'],\n ['c', 'h', 'â', 'u'],\n ['n', 'g', 'u', 'ụ'],\n ['B', 'ế', 'n'],\n ['T', 'T', 'e'],\n ['ấ', 'p'],\n ['x', 'ã'],\n ['l', 'à'],\n ['ở'],\n ['1']]"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "CTCStringTransform(vocab)(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}