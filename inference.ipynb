{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.encoder import Encoder\n",
    "from model.decoder import Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from dataset import get_dataset, collate_fn, vocab_size, int2char, char2int, SOS_CHAR, EOS_CHAR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from utils import ScaleImageByHeight, PaddingWidth, AverageMeter, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size': 32,\n",
    "    'hidden_size': 256,\n",
    "    'attn_size': 256,\n",
    "    'max_length': 10,\n",
    "    'n_epochs_decrease_lr': 15,\n",
    "    'start_learning_rate': 1e-5,  # NOTE: paper start with 1e-8\n",
    "    'end_learning_rate': 1e-11,\n",
    "    'depth': 4,\n",
    "    'n_blocks': 3,\n",
    "    'growth_rate': 96,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = config['max_length']\n",
    "CKPT_DIR = './ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = torch.load(os.path.join(CKPT_DIR, 'BEST_weights.pt'), map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    transforms.Grayscale(3),\n",
    "    ScaleImageByHeight(128),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = get_dataset('test', image_transform)\n",
    "test_loader = DataLoader(test_data, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(config['depth'], config['n_blocks'], config['growth_rate'])\n",
    "encoder.load_state_dict(info['encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(encoder.n_features,\n",
    "                  config['hidden_size'], vocab_size, config['attn_size'])\n",
    "decoder.load_state_dict(info['decoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, targets, targets_onehot, lengths = next(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imgs[0].squeeze().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join([int2char[x.item()] for x in targets[:,0].squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_input = torch.zeros(1, config['batch_size'], vocab_size)\n",
    "start_input[0,0, char2int[SOS_CHAR]] = 1\n",
    "start_input = start_input.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "#     for i, (imgs, targets, targets_onehot, lengths) in enumerate(val_loader):\n",
    "    imgs = imgs.to(device)\n",
    "    img_features = encoder(imgs)\n",
    "    outputs, weights = decoder.greedy(img_features, start_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, index = outputs.topk(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = index.squeeze().transpose(0, 1) # [B, T]\n",
    "predicts_str = []\n",
    "for predict in predicts:\n",
    "    s = [int2char[x.item()] for x in predict]\n",
    "    try:\n",
    "        eos_index = s.index(EOS_CHAR) + 1\n",
    "    except ValueError:\n",
    "        eos_index = len(s)\n",
    "    predicts_str.append(s[:eos_index])\n",
    "\n",
    "predicts_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = imgs.size(2), imgs.size(3)\n",
    "print(img_rows, img_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(predicts)\n",
    "length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 1\n",
    "sample_image, sample_predict, sample_weigth = imgs[sample_index], predicts_str[sample_index], weights[:, [sample_index]]\n",
    "fig, axeses = plt.subplots(len(sample_predict), figsize=(15,15), sharex=True, sharey=True)\n",
    "\n",
    "for i, axes in enumerate(axeses.ravel()):\n",
    "    weight = weights[i].reshape(-1, config['batch_size'], img_rows // 16, img_cols // 16) # 16 is factor that DenseNet reduce the original image size\n",
    "    weight_numpy = weight.cpu().numpy()[:,sample_index,:].squeeze()\n",
    "    weight_image = skimage.transform.resize(weight_numpy, (img_rows, img_cols))\n",
    "    \n",
    "    img = sample_image.squeeze().permute(1,2,0).cpu().numpy()[:,:,0]\n",
    "    \n",
    "    alpha = 0.5\n",
    "    blend = img * alpha + weight_image * (1-alpha)\n",
    "    \n",
    "    axes.set_title(sample_predict[i])\n",
    "    axes.imshow(blend, cmap='spring')\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calc CER, WER on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance as ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_test = open('./log_test.txt', 'w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_characters = 0\n",
    "total_words = 0\n",
    "CE = 0\n",
    "WE = 0\n",
    "log_interval = 10\n",
    "\n",
    "# t = tqdm(test_loader)\n",
    "t = test_loader\n",
    "with torch.no_grad():\n",
    "    for i, (imgs, targets, targets_onehot, lengths) in enumerate(t):\n",
    "        print(f'[{i}]/[{len(t)}]', file=log_test)\n",
    "        log_test.flush()\n",
    "        batch_size = imgs.size(0)\n",
    "        \n",
    "        start_input = torch.zeros(1, batch_size, vocab_size)\n",
    "        start_input[0,0, char2int[SOS_CHAR]] = 1\n",
    "        start_input = start_input.to(device)\n",
    "        imgs = imgs.to(device)\n",
    "        \n",
    "        img_features = encoder(imgs)\n",
    "        outputs, weights = decoder.greedy(img_features, start_input)\n",
    "        \n",
    "        _, index = outputs.topk(1, -1)\n",
    "        predicts = index.squeeze().transpose(0, 1) # [B, T]\n",
    "        predicts_str = []\n",
    "        for predict in predicts:\n",
    "            s = [int2char[x.item()] for x in predict]\n",
    "            try:\n",
    "                eos_index = s.index(EOS_CHAR) + 1\n",
    "            except ValueError:\n",
    "                eos_index = len(s)\n",
    "            predicts_str.append(s[:eos_index])\n",
    "\n",
    "        targets_str = []\n",
    "        for target in targets.transpose(0, 1).squeeze():\n",
    "            s = [int2char[x.item()] for x in target]\n",
    "            try:\n",
    "                eos_index = s.index(EOS_CHAR) + 1\n",
    "            except ValueError:\n",
    "                eos_index = len(s)\n",
    "            targets_str.append(s[:eos_index])\n",
    "        \n",
    "        assert len(predicts_str) == len(targets_str)\n",
    "        for i in range(len(predicts_str)):\n",
    "            CE += ed.distance(predicts_str[i], targets_str[i])\n",
    "        total_characters += lengths.sum().item()\n",
    "        \n",
    "        for i in range(len(predicts_str)):\n",
    "            WE += 1 if np.array_equal(np.array(predicts_str[i]), np.array(targets_str[i])) else 0\n",
    "        total_words += len(predicts_str)\n",
    "        \n",
    "#         t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CER = CE / total_characters\n",
    "WER = WE / total_words\n",
    "print('CER', CER, file=log_test)\n",
    "print('WER', WER, file=log_test)\n",
    "log_test.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
